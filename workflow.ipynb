{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "import lightning as L\n",
    "from lightning.pytorch.utilities.types import STEP_OUTPUT\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from typing import Any\n",
    "import wandb\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead9c28d-b5c8-4dea-9ced-d16a50230cb2",
   "metadata": {},
   "source": [
    "# TransPath Model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6739d1-c7b7-4467-8556-24aa36f99c1c",
   "metadata": {},
   "source": [
    "![architecture](./images/architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c712b3a-80d3-498c-bb3b-fdd3d4b2ae8e",
   "metadata": {},
   "source": [
    "## ResNet Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08a0646f-2a9e-400c-b43e-5866d5a67144",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        out_channels = in_channels if out_channels is None else out_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.norm1 = nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)\n",
    "        self.norm2 = nn.GroupNorm(num_groups=32, num_channels=out_channels, eps=1e-6, affine=True)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.idConv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "\n",
    "        h = self.norm1(h)\n",
    "        h = self.silu(h)\n",
    "        h = self.conv1(h)\n",
    "\n",
    "        h = self.norm2(h)\n",
    "        h = self.silu(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.conv2(h)\n",
    "\n",
    "        x = self.idConv(x)\n",
    "\n",
    "        return x + h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0724936-e6f8-4546-9ee6-10e160fb86af",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e050504a-d64d-4aaa-b3da-12ea5f7845e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Downsample(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            in_channels,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, scale_factor=0.5, mode=\"nearest\")\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51c96a79-69cb-4e8b-a417-cafccede9750",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, downsample_steps, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Conv2d(\n",
    "                in_channels, \n",
    "                hidden_channels, \n",
    "                kernel_size=5, \n",
    "                stride=1, \n",
    "                padding=2\n",
    "            )\n",
    "        ])\n",
    "        for _ in range(downsample_steps):\n",
    "            self.layers.append(\n",
    "                nn.Sequential(\n",
    "                    ResNetBlock(hidden_channels, hidden_channels, dropout),\n",
    "                    Downsample(hidden_channels)\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc309fb-2b0b-41df-ad7b-8371c6ffb579",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d1bd82f-f201-4029-b8b8-9d160545a25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Upsample(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            in_channels,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, scale_factor=2.0, mode=\"nearest\")\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f559cd5-b9ef-40a8-af22-89654b103f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, upsample_steps, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(upsample_steps):\n",
    "            self.layers.append(\n",
    "                nn.Sequential(\n",
    "                    ResNetBlock(hidden_channels, hidden_channels, dropout),\n",
    "                    Upsample(hidden_channels)\n",
    "                )\n",
    "            )\n",
    "        self.norm = nn.GroupNorm(num_groups=32, num_channels=hidden_channels, eps=1e-6, affine=True)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.conv_out = nn.Conv2d(\n",
    "            hidden_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.silu(x)\n",
    "        x = self.conv_out(x)\n",
    "        return torch.tanh(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23028786-8c4a-49d1-a9c5-2202c6865936",
   "metadata": {},
   "source": [
    "## Positional Embedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6566bb75-2033-475f-afb3-e45f8ef45730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_grid(resolution, max_v=1.):\n",
    "    \"\"\"\n",
    "    :param resolution: tuple of 2 numbers\n",
    "    :return: grid for positional embeddings built on input resolution\n",
    "    \"\"\"\n",
    "    ranges = [np.linspace(0., max_v, num=res) for res in resolution]\n",
    "    grid = np.meshgrid(*ranges, sparse=False, indexing=\"ij\")\n",
    "    grid = np.stack(grid, axis=-1)\n",
    "    grid = np.reshape(grid, [resolution[0], resolution[1], -1])\n",
    "    grid = np.expand_dims(grid, axis=0)\n",
    "    grid = grid.astype(np.float32)\n",
    "    return np.concatenate([grid, max_v - grid], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1a855d0-f928-466b-a5bf-88d32ca4b9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosEmbeds(nn.Module):\n",
    "    def __init__(self, hidden_size, resolution):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(4, hidden_size)\n",
    "        self.grid = nn.Parameter(Tensor(build_grid(resolution)), requires_grad=False)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        pos_emb = self.linear(self.grid).moveaxis(3, 1)\n",
    "        return inputs + pos_emb\n",
    "    \n",
    "    def change_resolution(self, resolution, max_v):\n",
    "        self.grid = nn.Parameter(Tensor(build_grid(resolution, max_v)), requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21115de8-f6a0-4974-a970-b88b0a7a180d",
   "metadata": {},
   "source": [
    "## FeedForward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c225be2c-05d3-4213-94dc-3e89957dea7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels=None, dropout=0.2):\n",
    "        super().__init__()\n",
    "        out_channels = in_channels if out_channels is None else out_channels\n",
    "\n",
    "        self.linear1 = nn.Linear(in_channels, hidden_channels)\n",
    "        self.linear2 = nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba54ca9-e291-4f46-98e7-d3587b214e5e",
   "metadata": {},
   "source": [
    "## Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11f46691-972e-4ddf-9c35-9e72cd2c44fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicTransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.3, context_dim=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn1 = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.attn2 = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, kdim=context_dim, vdim=context_dim, batch_first=True)\n",
    "        self.ff = FeedForward(embed_dim, embed_dim * 4, dropout=dropout)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x, context=None):\n",
    "        h1 = self.norm1(x)\n",
    "        h1, _ = self.attn1(h1, h1, h1, need_weights=False)\n",
    "        x = h1 + x\n",
    "        \n",
    "        h2 = self.norm2(x)\n",
    "        context = h2 if context is None else context\n",
    "        h2, _ = self.attn2(h2, context, context, need_weights=False)\n",
    "        x = h2 + x\n",
    "\n",
    "        h3 = self.norm3(x)\n",
    "        x = self.ff(h3) + x\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd8979c4-ac4f-4266-acd1-50357eb92a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer block for image-like data.\n",
    "    First, project the input (aka embedding)\n",
    "    and reshape to b, t, d.\n",
    "    Then apply standard transformer action.\n",
    "    Finally, reshape to image\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, num_heads,\n",
    "                 depth=4, dropout=0.3, context_dim=None):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        \n",
    "        self.norm = nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [BasicTransformerBlock(in_channels, num_heads, dropout=dropout, context_dim=context_dim)\n",
    "                for d in range(depth)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, context=None):\n",
    "        # note: if no context is given, cross-attention defaults to self-attention\n",
    "        b, c, h, w = x.shape\n",
    "        f = x\n",
    "        f = self.norm(f)\n",
    "        f = rearrange(f, 'b c h w -> b (h w) c')\n",
    "        for block in self.transformer_blocks:\n",
    "            f = block(f, context=context)\n",
    "        f = rearrange(f, 'b (h w) c -> b c h w', h=h, w=w)\n",
    "        return f + x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e4a4b6-3e1f-485c-82ac-1d55b9d1a49e",
   "metadata": {},
   "source": [
    "# TransPath Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd09ecb1-b577-4cce-80aa-e545e0dbb733",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransPathModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                in_channels=2, \n",
    "                out_channels=1, \n",
    "                hidden_channels=64,\n",
    "                attn_blocks=4,\n",
    "                attn_heads=4,\n",
    "                cnn_dropout=0.15,\n",
    "                attn_dropout=0.15,\n",
    "                downsample_steps=3, \n",
    "                resolution=(64, 64)):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder(in_channels, hidden_channels, downsample_steps, cnn_dropout)\n",
    "        self.decoder = Decoder(hidden_channels, out_channels, downsample_steps, cnn_dropout)\n",
    "        \n",
    "        self.encoder_pos = PosEmbeds(\n",
    "            hidden_channels, \n",
    "            (resolution[0] // 2**downsample_steps, resolution[1] // 2**downsample_steps)\n",
    "        )\n",
    "        self.decoder_pos = PosEmbeds(\n",
    "            hidden_channels, \n",
    "            (resolution[0] // 2**downsample_steps, resolution[1] // 2**downsample_steps)\n",
    "        )\n",
    "\n",
    "        self.transformer = SpatialTransformer(\n",
    "            hidden_channels, \n",
    "            attn_heads,\n",
    "            attn_blocks, \n",
    "            attn_dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.encoder_pos(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.decoder_pos(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a281b66d-7825-4c10-a1b9-bbe2b71f9bfd",
   "metadata": {},
   "source": [
    "## Training Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8e70f0b-7353-4a9a-85b7-f9844b30c305",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransPathLit(L.LightningModule):\n",
    "    def __init__(self, model: nn.Module, mode: str='f', learning_rate: float=1e-4, weight_decay: float=0.0) -> None:\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.model = model\n",
    "        self.mode = mode\n",
    "        self.loss = nn.L1Loss() if mode == 'h' else nn.MSELoss()\n",
    "        self.k = 64*64 if mode == 'h' else 1\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def training_step(\n",
    "        self, batch: tuple[Tensor, Tensor, Tensor, Tensor], batch_idx: int\n",
    "    ) -> STEP_OUTPUT:\n",
    "        map_design, start, goal, gt_hmap = batch\n",
    "        inputs = torch.cat([map_design, start + goal], dim=1) if self.mode in ('f', 'nastar') else torch.cat([map_design, goal], dim=1)\n",
    "        predictions = self.model(inputs)\n",
    "        loss = self.loss((predictions + 1) / 2 * self.k, gt_hmap)\n",
    "        self.log(f'train_loss', loss, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(\n",
    "        self, batch: tuple[Tensor, Tensor, Tensor, Tensor], batch_idx: int\n",
    "    ) -> STEP_OUTPUT | None:\n",
    "        map_design, start, goal, gt_hmap = batch\n",
    "        inputs = torch.cat([map_design, start + goal], dim=1) if self.mode in ('f', 'nastar') else torch.cat([map_design, goal], dim=1)\n",
    "        predictions = self.model(inputs)\n",
    "        loss = self.loss((predictions + 1) / 2 * self.k, gt_hmap)\n",
    "        self.log(f'val_loss', loss, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self) -> dict[str, Any]:\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, max_lr=self.learning_rate, total_steps=self.trainer.estimated_stepping_batches\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": scheduler\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8386804d-e5eb-42db-8da1-d6e35e4a8699",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PathLogger(L.Callback):\n",
    "    def __init__(self, val_batch, num_samples=10, mode='f'):\n",
    "        super().__init__()\n",
    "        map_design, start, goal, gt_hmap = val_batch[:num_samples]\n",
    "        inputs = torch.cat([map_design, start + goal], dim=1) if mode == 'f' else torch.cat([map_design, goal], dim=1)\n",
    "        self.val_samples = inputs[:num_samples]\n",
    "        if mode == 'f':\n",
    "            self.hm = gt_hmap[:num_samples]\n",
    "        elif mode == 'h':\n",
    "            self.hm =  (gt_hmap / gt_hmap.amax(dim=(2, 3), keepdim=True))[:num_samples]\n",
    "        else:\n",
    "            self.hm = gt_hmap[:num_samples]\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, lightning_module):\n",
    "        val_samples = self.val_samples.to(device=lightning_module.device)\n",
    "        prediction = (lightning_module.model(val_samples) + 1) / 2\n",
    "        if lightning_module.mode == 'h':\n",
    "            prediction = prediction * 64 * 64\n",
    "\n",
    "        trainer.logger.experiment.log({\n",
    "            'data': [wandb.Image(x) for x in torch.cat([self.val_samples, self.hm], dim=1)],\n",
    "            'predictions': [wandb.Image(x) for x in torch.cat([val_samples, prediction], dim=1)]\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14070d7-16ee-4e67-825a-1d099374b582",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4ff4530-0e8f-4667-8fbb-7bb0f72bb13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridData(Dataset):\n",
    "    \"\"\"\n",
    "    'mode' argument defines type of ground truth values:\n",
    "        f - focal values\n",
    "        h - absolute ideal heuristic values\n",
    "        cf - correction factor values\n",
    "    \"\"\"\n",
    "    def __init__(self, path, mode='f', clip_value=0.95, img_size=64):\n",
    "        self.img_size = img_size\n",
    "        self.clip_v = clip_value\n",
    "        self.mode = mode\n",
    "\n",
    "        self.maps   = np.load(os.path.join(path,    'maps.npy'),    mmap_mode='c')\n",
    "        self.goals  = np.load(os.path.join(path,    'goals.npy'),   mmap_mode='c')\n",
    "        self.starts = np.load(os.path.join(path,    'starts.npy'),  mmap_mode='c')\n",
    "        \n",
    "        file_gt = {'f' : 'focal.npy', 'h':'abs.npy', 'cf': 'cf.npy'}[mode]\n",
    "        self.gt_values = np.load(os.path.join(path, file_gt), mmap_mode='c')\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.gt_values)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        gt_ = torch.from_numpy(self.gt_values[idx].astype('float32'))\n",
    "        if self.mode == 'f':\n",
    "            gt_=  torch.where( gt_ >= self.clip_v, gt_ , torch.zeros_like( torch.from_numpy(self.gt_values[idx])))\n",
    "        return (torch.from_numpy(self.maps[idx].astype('float32')), \n",
    "                torch.from_numpy(self.starts[idx].astype('float32')), \n",
    "                torch.from_numpy(self.goals[idx].astype('float32')), \n",
    "                gt_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bed5b24d-ff1c-4f5b-9791-691ade4b6424",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = './TransPath_data'\n",
    "mode = 'f'\n",
    "batch_size = 256\n",
    "max_epochs = 200\n",
    "learning_rate = 4e-4\n",
    "weight_decay = 0.0\n",
    "limit_train_batches = None\n",
    "limit_val_batches = None\n",
    "proj_name = 'TransPath_runs'\n",
    "run_name = 'default'\n",
    "accelerator = \"cuda\"\n",
    "devices = [3]\n",
    "torch.set_default_device(torch.device(f\"cuda:{devices[-1]}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "915559fb-958e-47bc-a0d7-746368086ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment next lines if you want to get full dataset\n",
    "# ! python download.py\n",
    "# dataset_dir = './TransPath_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5aa10a1a-e6aa-4282-8bb2-13427f830581",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = GridData(\n",
    "        path=f'{dataset_dir}/train',\n",
    "        mode=mode\n",
    "    )\n",
    "val_data = GridData(\n",
    "        path=f'{dataset_dir}/val',\n",
    "        mode=mode\n",
    "    )\n",
    "resolution = (train_data.img_size, train_data.img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "967677a4-992f-4969-8b0f-3802480ae10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "train_dataloader = DataLoader(\n",
    "        train_data, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=True, \n",
    "        # num_workers=multiprocessing.cpu_count(), freezes jupyter notebook\n",
    "        pin_memory=True,\n",
    "        generator=torch.Generator(device=f'cuda:{devices[-1]}'),\n",
    "    )\n",
    "val_dataloader = DataLoader(\n",
    "        val_data, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=False, \n",
    "        # num_workers=multiprocessing.cpu_count(), freezes jupyter notebook\n",
    "        pin_memory=True\n",
    "    )\n",
    "samples = next(iter(val_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3906e969-1018-419b-9b44-fbca5842e800",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3632972-b79e-4dca-8db8-de725a316e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/drozdovdan/miniconda3/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "callback = PathLogger(samples, mode=mode)\n",
    "checkpoints = ModelCheckpoint(dirpath='checkpoints/', filename='{mode}-{epoch}-{timestr}', every_n_epochs=50)\n",
    "wandb_logger = WandbLogger(project=proj_name, name=f'{run_name}_{mode}', log_model='all')\n",
    "\n",
    "model = TransPathModel()\n",
    "lit_module = TransPathLit(\n",
    "        model=model,\n",
    "        mode=mode,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "trainer = L.Trainer(\n",
    "        logger=wandb_logger,\n",
    "        accelerator=accelerator,\n",
    "        devices=devices,\n",
    "        max_epochs=max_epochs,\n",
    "        deterministic=False,\n",
    "        limit_train_batches=limit_train_batches,\n",
    "        limit_val_batches=limit_val_batches,\n",
    "        callbacks=[callback, checkpoints],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2025b7-7fea-4f3a-b5e0-d32d76911912",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A100-PCIE-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdaniil-drozdovjr\u001b[0m (\u001b[33mdaniil-drozdovjr-saint-petersburg-state-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20241217_020700-52hyw73y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/daniil-drozdovjr-saint-petersburg-state-university/TransPath_runs/runs/52hyw73y' target=\"_blank\">default_f</a></strong> to <a href='https://wandb.ai/daniil-drozdovjr-saint-petersburg-state-university/TransPath_runs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/daniil-drozdovjr-saint-petersburg-state-university/TransPath_runs' target=\"_blank\">https://wandb.ai/daniil-drozdovjr-saint-petersburg-state-university/TransPath_runs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/daniil-drozdovjr-saint-petersburg-state-university/TransPath_runs/runs/52hyw73y' target=\"_blank\">https://wandb.ai/daniil-drozdovjr-saint-petersburg-state-university/TransPath_runs/runs/52hyw73y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/storage/drozdovdan/miniconda3/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=255` in the `DataLoader` to improve performance.\n",
      "\n",
      "  | Name  | Type           | Params | Mode \n",
      "-------------------------------------------------\n",
      "0 | model | TransPathModel | 963 K  | train\n",
      "1 | loss  | MSELoss        | 0      | train\n",
      "-------------------------------------------------\n",
      "962 K     Trainable params\n",
      "512       Non-trainable params\n",
      "963 K     Total params\n",
      "3.854     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0cfc96fa6d745e08e729701fc775f27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                      | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/drozdovdan/miniconda3/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=255` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8ce1b52789648a98fbf2bcea1fc4137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecca69c624c1439f93da6dfcc170d560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(lit_module, train_dataloader, val_dataloader)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dcb34d-adaf-4dee-940c-7123c2834a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_dir = './weights/'\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "torch.save(model.state_dict(), f'{weights_dir}/model_{timestr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e280ba43-549d-4e9c-a963-592123e9e870",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ea5508-4e5f-45a6-aa66-bda26069bc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './weights/model_20241207-224531'\n",
    "indices = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "\n",
    "eval_model = TransPathModel()\n",
    "eval_model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "eval_model.eval()\n",
    "\n",
    "test_data = GridData(\n",
    "        path=f'{dataset_dir}/test',\n",
    "        mode=mode\n",
    "    )[indices]\n",
    "\n",
    "map_design, start, goal, gt_hmap = test_data\n",
    "inputs = torch.cat([map_design, start + goal], dim=1) if mode in ('f', 'nastar') else torch.cat([map_design, goal], dim=1)\n",
    "predictions = (eval_model(inputs) + 1) / 2\n",
    "for i in range(len(indices)):\n",
    "    plt.imshow(gt_hmap[i, 0], cmap='gray')\n",
    "    plt.show()\n",
    "    plt.imshow(predictions[i, 0].cpu().detach().numpy(), cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e28071-bce8-4465-965b-dae17a3a2d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_test_data = GridData(\n",
    "        path=f'./eval_data/eval',\n",
    "        mode=mode\n",
    "    )[indices]\n",
    "\n",
    "map_design, start, goal, gt_hmap = my_test_data\n",
    "inputs = torch.cat([map_design, start + goal], dim=1) if mode in ('f', 'nastar') else torch.cat([map_design, goal], dim=1)\n",
    "predictions = (eval_model(inputs) + 1) / 2\n",
    "\n",
    "for i in range(len(indices)):\n",
    "    plt.imshow(1 - (map_design[i, 0] + 5 * start[i, 0] + 5 * goal[i, 0]), cmap='gray')\n",
    "    plt.show()\n",
    "    plt.imshow(predictions[i, 0].cpu().detach().numpy(), cmap='gray')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
