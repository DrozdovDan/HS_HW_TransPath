{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "import lightning as L\n",
    "from lightning.pytorch.utilities.types import STEP_OUTPUT\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead9c28d-b5c8-4dea-9ced-d16a50230cb2",
   "metadata": {},
   "source": [
    "# TransPath Model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6739d1-c7b7-4467-8556-24aa36f99c1c",
   "metadata": {},
   "source": [
    "![architecture](./images/architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c712b3a-80d3-498c-bb3b-fdd3d4b2ae8e",
   "metadata": {},
   "source": [
    "## ResNet Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08a0646f-2a9e-400c-b43e-5866d5a67144",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        out_channels = in_channels if out_channels is None else out_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.norm1 = nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)\n",
    "        self.norm2 = nn.GroupNorm(num_groups=32, num_channels=out_channels, eps=1e-6, affine=True)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.idConv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "\n",
    "        h = self.norm1(h)\n",
    "        h = self.silu(h)\n",
    "        h = self.conv1(h)\n",
    "\n",
    "        h = self.norm2(h)\n",
    "        h = self.silu(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.conv2(h)\n",
    "\n",
    "        x = self.idConv(x)\n",
    "\n",
    "        return x + h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0724936-e6f8-4546-9ee6-10e160fb86af",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e050504a-d64d-4aaa-b3da-12ea5f7845e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Downsample(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            in_channels,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, scale_factor=0.5, mode=\"nearest\")\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51c96a79-69cb-4e8b-a417-cafccede9750",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, downsample_steps, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Conv2d(\n",
    "                in_channels, \n",
    "                hidden_channels, \n",
    "                kernel_size=5, \n",
    "                stride=1, \n",
    "                padding=2\n",
    "            )\n",
    "        ])\n",
    "        for _ in range(downsample_steps):\n",
    "            self.layers.append(\n",
    "                nn.Sequential(\n",
    "                    ResnetBlock(hidden_channels, hidden_channels, dropout),\n",
    "                    Downsample(hidden_channels)\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc309fb-2b0b-41df-ad7b-8371c6ffb579",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d1bd82f-f201-4029-b8b8-9d160545a25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Upsample(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            in_channels,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, scale_factor=2.0, mode=\"nearest\")\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f559cd5-b9ef-40a8-af22-89654b103f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, upsample_steps, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(upsample_steps):\n",
    "            self.layers.append(\n",
    "                nn.Sequential(\n",
    "                    ResnetBlock(hidden_channels, hidden_channels, dropout),\n",
    "                    Upsample(hidden_channels)\n",
    "                )\n",
    "            )\n",
    "        self.norm = nn.GroupNorm(num_groups=32, num_channels=hidden_channels, eps=1e-6, affine=True)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.conv_out = nn.Conv2d(\n",
    "            hidden_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.silu(x)\n",
    "        x = self.conv_out(x)\n",
    "        return torch.tanh(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23028786-8c4a-49d1-a9c5-2202c6865936",
   "metadata": {},
   "source": [
    "## Positional Embedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6566bb75-2033-475f-afb3-e45f8ef45730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_grid(resolution, max_v=1.):\n",
    "    \"\"\"\n",
    "    :param resolution: tuple of 2 numbers\n",
    "    :return: grid for positional embeddings built on input resolution\n",
    "    \"\"\"\n",
    "    ranges = [np.linspace(0., max_v, num=res) for res in resolution]\n",
    "    grid = np.meshgrid(*ranges, sparse=False, indexing=\"ij\")\n",
    "    grid = np.stack(grid, axis=-1)\n",
    "    grid = np.reshape(grid, [resolution[0], resolution[1], -1])\n",
    "    grid = np.expand_dims(grid, axis=0)\n",
    "    grid = grid.astype(np.float32)\n",
    "    return np.concatenate([grid, max_v - grid], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1a855d0-f928-466b-a5bf-88d32ca4b9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosEmbeds(nn.Module):\n",
    "    def __init__(self, hidden_size, resolution):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(4, hidden_size)\n",
    "        self.grid = nn.Parameter(Tensor(build_grid(resolution)), requires_grad=False)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        pos_emb = self.linear(self.grid).moveaxis(3, 1)\n",
    "        return inputs + pos_emb\n",
    "    \n",
    "    def change_resolution(self, resolution, max_v):\n",
    "        self.grid = nn.Parameter(Tensor(build_grid(resolution, max_v)), requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21115de8-f6a0-4974-a970-b88b0a7a180d",
   "metadata": {},
   "source": [
    "## FeedForward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c225be2c-05d3-4213-94dc-3e89957dea7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels=None, dropout=0.2):\n",
    "        super().__init__()\n",
    "        out_channels = in_channels if out_channels is None else out_channels\n",
    "\n",
    "        self.linear1 = nn.Linear(in_channels, hidden_channels)\n",
    "        self.linear2 = nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba54ca9-e291-4f46-98e7-d3587b214e5e",
   "metadata": {},
   "source": [
    "## Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11f46691-972e-4ddf-9c35-9e72cd2c44fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicTransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.3, context_dim=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn1 = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.attn2 = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, kdim=context_dim, vdim=context_dim, batch_first=True)\n",
    "        self.ff = FeedForward(embed_dim, embed_dim * 4, dropout=dropout)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x, context=None):\n",
    "        h1 = self.norm1(x)\n",
    "        x = self.attn1(h1, h1, h1, need_weights=False) + x\n",
    "        \n",
    "        h2 = self.norm2(x)\n",
    "        context = h2 if context is None else context\n",
    "        x = self.attn2(h2, context, context, need_weights=False) + x\n",
    "\n",
    "        h3 = self.norm3(x)\n",
    "        x = self.ff(h3) + x\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd8979c4-ac4f-4266-acd1-50357eb92a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer block for image-like data.\n",
    "    First, project the input (aka embedding)\n",
    "    and reshape to b, t, d.\n",
    "    Then apply standard transformer action.\n",
    "    Finally, reshape to image\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, num_heads,\n",
    "                 depth=4, dropout=0.3, context_dim=None):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        \n",
    "        self.norm = nn.GroupNorm(num_groups=32, num_channels=hidden_channels, eps=1e-6, affine=True)\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [BasicTransformerBlock(in_channels, num_heads, dropout=dropout, kdim=context_dim, vdim=context_dim)\n",
    "                for d in range(depth)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, context=None):\n",
    "        # note: if no context is given, cross-attention defaults to self-attention\n",
    "        b, c, h, w = x.shape\n",
    "        f = x\n",
    "        f = self.norm(f)\n",
    "        f = rearrange(f, 'b c h w -> b, (h w) c')\n",
    "        for block in self.transformer_blocks:\n",
    "            f = block(f, context=context)\n",
    "        f = rearrange(f, 'b (h w) c -> b c h w', h=h, w=w)\n",
    "        return f + x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e4a4b6-3e1f-485c-82ac-1d55b9d1a49e",
   "metadata": {},
   "source": [
    "# TransPath Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd09ecb1-b577-4cce-80aa-e545e0dbb733",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransPathModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                in_channels=2, \n",
    "                out_channels=1, \n",
    "                hidden_channels=64,\n",
    "                attn_blocks=4,\n",
    "                attn_heads=4,\n",
    "                cnn_dropout=0.15,\n",
    "                attn_dropout=0.15,\n",
    "                downsample_steps=3, \n",
    "                resolution=(64, 64)):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder(in_channels, hidden_channels, downsample_steps, cnn_dropout)\n",
    "        self.decoder = Decoder(hidden_channels, out_channels, downsample_steps, cnn_dropout)\n",
    "        \n",
    "        self.encoder_pos = PosEmbeds(\n",
    "            hidden_channels, \n",
    "            (resolution[0] // 2**downsample_steps, resolution[1] // 2**downsample_steps)\n",
    "        )\n",
    "        self.decoder_pos = PosEmbeds(\n",
    "            hidden_channels, \n",
    "            (resolution[0] // 2**downsample_steps, resolution[1] // 2**downsample_steps)\n",
    "        )\n",
    "\n",
    "        self.transformer = SpatialTransformer(\n",
    "            hidden_channels, \n",
    "            attn_heads,\n",
    "            attn_blocks, \n",
    "            attn_dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.encoder_pos(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.decoder_pos(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a281b66d-7825-4c10-a1b9-bbe2b71f9bfd",
   "metadata": {},
   "source": [
    "## Training Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8e70f0b-7353-4a9a-85b7-f9844b30c305",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransPathLit(L.LightningModule):\n",
    "    def __init__(self, model: nn.Module, mode: str='f', learning_rate: float=1e-4, weight_decay: float=0.0) -> None:\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.model = model\n",
    "        self.mode = mode\n",
    "        self.loss = nn.L1Loss() if mode == 'h' else nn.MSELoss()\n",
    "        self.k = 64*64 if mode == 'h' else 1\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def training_step(\n",
    "        self, batch: tuple[Tensor, Tensor, Tensor, Tensor], batch_idx: int\n",
    "    ) -> STEP_OUTPUT:\n",
    "        map_design, start, goal, gt_hmap = batch\n",
    "        inputs = torch.cat([map_design, start + goal], dim=1) if self.mode in ('f', 'nastar') else torch.cat([map_design, goal], dim=1)\n",
    "        predictions = self.model(inputs)\n",
    "        loss = self.loss((predictions + 1) / 2 * self.k, gt_hmap)\n",
    "        self.log(f'train_loss', loss, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(\n",
    "        self, batch: tuple[Tensor, Tensor, Tensor, Tensor], batch_idx: int\n",
    "    ) -> STEP_OUTPUT | None:\n",
    "        map_design, start, goal, gt_hmap = batch\n",
    "        inputs = torch.cat([map_design, start + goal], dim=1) if self.mode in ('f', 'nastar') else torch.cat([map_design, goal], dim=1)\n",
    "        predictions = self.model(inputs)\n",
    "        loss = self.loss((predictions + 1) / 2 * self.k, gt_hmap)\n",
    "        self.log(f'val_loss', loss, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self) -> dict[str, Any]:\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, max_lr=self.learning_rate, total_steps=self.trainer.estimated_stepping_batches\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": scheduler\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8386804d-e5eb-42db-8da1-d6e35e4a8699",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
